{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20a6c8d-b0ec-409f-b52e-8ef9135881bb",
   "metadata": {},
   "source": [
    "# MoleculeDiffusionTransformer: Basic model setups\n",
    "\n",
    "Reference:\n",
    "\n",
    "#### Generative Discovery of Novel Chemical Designs using Diffusion Modeling and Transformer Architectures with Application to Deep Eutectic Solvents \n",
    "\n",
    "Rachel K. Luu1,2, Marcin Wysokowski1 , Markus J. Buehler1,3*\n",
    "\n",
    "1 Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "2 Department of Materials Science and Engineering, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "3 Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "\n",
    "mbuehler@MIT.EDU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf88c6f8-7aa3-45d1-8984-414638d7b910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71d795-d859-4c48-aceb-ef25b1099ebb",
   "metadata": {},
   "source": [
    "## Generative inverse diffusion mode: Basic model setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b598b455-bc6d-41ef-b06a-73582d8ad62d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unet type:  cfg\n",
      "[<class 'MoleculeDiffusion.diffusion.VDiffusion'>, <class 'MoleculeDiffusion.diffusion.KDiffusion'>, <class 'MoleculeDiffusion.diffusion.VKDiffusion'>, <class 'MoleculeDiffusion.diffusion.KDiffusion_mod'>]\n",
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "from MoleculeDiffusion import QMDiffusion \n",
    "\n",
    "device='cpu'\n",
    "max_length = 64\n",
    "pred_dim=16 #dimension equals number of unique tokens\n",
    "context_embedding_max_length=12 #dimension equals length of conditioning, i.e. number of molecular features to be considered\n",
    "\n",
    "model =QMDiffusion( \n",
    "        max_length=max_length,#length of predicted results, i.e. max length of the SMILES string\n",
    "        pred_dim=pred_dim,\n",
    "        channels=64,\n",
    "        unet_type='cfg', #'base', #'cfg',\n",
    "        context_embedding_max_length=context_embedding_max_length,#length of conditioning \n",
    "        pos_emb_fourier=True,\n",
    "        pos_emb_fourier_add=False,\n",
    "        text_embed_dim = 64,\n",
    "        embed_dim_position=64,\n",
    "        )  .to(device)\n",
    "\n",
    "sequences= torch.randn(4, context_embedding_max_length ).to (device) #conditioning sequence; note, max_text_len=12, \n",
    "output=torch.randint (0,pred_dim, (4, pred_dim , max_length)).to(device).float() #batch, number of tokens, length (length is flexible)\n",
    " \n",
    "loss=model(sequences=sequences, #conditioning sequence (set of floating points)\n",
    "           output=output, #desired result (e.g. one-hot encoded sequence\n",
    "        )\n",
    "loss.backward()\n",
    "loss\n",
    "\n",
    "#Generate\n",
    "generated=model.sample (sequences,\n",
    "              device,\n",
    "              cond_scale=1.,\n",
    "              timesteps=64,\n",
    "              clamp=False,\n",
    "              )\n",
    " \n",
    "print (generated.shape) #(b, pred_dim, max_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3a793-2e8b-49fa-a8ba-0fe58b0717ec",
   "metadata": {},
   "source": [
    "## Generative inverse transformer model: Basic model setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77949d73-b120-45bb-8f2e-7fdf01cd2790",
   "metadata": {},
   "source": [
    "#### Model that takes input in the form (batch, num_tokens, length); MSE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35743c5d-5339-4e92-9db0-481d1862cf23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 128 tokens...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8ad432710d4cbb808d49cf87872ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "#In this case, the input and output dimension is the same.\n",
    "\n",
    "from  MoleculeDiffusion import MoleculeTransformer \n",
    "\n",
    "logits_dim = 32 #number of tokens\n",
    "MolTrans = MoleculeTransformer(\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        logits_dim=logits_dim, #number of tokens, and also input/output dimension\n",
    "        dim_head = 16,\n",
    "        heads = 8,\n",
    "        dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        text_embed_dim = 32,\n",
    "        cond_drop_prob = 0.25,\n",
    "        max_text_len = 12, #max length of conditioning sequence\n",
    "        pos_fourier_graph_dim= 32, #entire graph fourier embedding, will be added to logits_dim\n",
    "        \n",
    ").cuda()\n",
    "\n",
    "sequences= torch.randn(4, 12 ).cuda() #conditioning sequence; note, max_text_len=12, \n",
    "output=torch.randint (0,logits_dim, (4, logits_dim , 128)).cuda().float() #batch, number of tokens, length (length is flexible)\n",
    " \n",
    "loss=MolTrans(\n",
    "        sequences=sequences,#conditioning sequence\n",
    "        output=output,\n",
    "        text_mask = None,\n",
    "        return_loss = True,\n",
    ")\n",
    "loss.backward()\n",
    "loss\n",
    "\n",
    "#Generate\n",
    "generated = MolTrans.generate(   sequences=sequences,#conditioning\n",
    "                                 tokens_to_generate=128, #can also generate less....\n",
    "                                 cond_scale = 1., temperature=1,  \n",
    "     )  \n",
    "print (generated.shape) #(b, number_tokens, tokens_to_generate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839d7ed-7195-4480-958c-89e80812093e",
   "metadata": {},
   "source": [
    "#### Model that takes input in the form of a sequence (batch, length); Cross Entropy loss (used in the paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06ba007-bece-4d03-b5a8-48c19977f744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 23])\n",
      "Since start token not provided, generating random token.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab42450cf9bd46c8bb037b0cc458c85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384d077be6274f0fa1f44a325fd70b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 33])\n"
     ]
    }
   ],
   "source": [
    "from   MoleculeDiffusion import MoleculeTransformerSequence, count_parameters\n",
    "logits_dim = 32 #number of tokens\n",
    "\n",
    "model = MoleculeTransformerSequence(\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        logits_dim=logits_dim, #number of tokens  \n",
    "        dim_head = 16,\n",
    "        heads = 8,\n",
    "        dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        text_embed_dim = 32,\n",
    "        cond_drop_prob = 0.25,\n",
    "        max_text_len = 12, #max length of conditioning sequence\n",
    "        pos_fourier_graph_dim= 32, #entire graph fourier embedding, will be added to logits_dim\n",
    "              \n",
    ").cuda()\n",
    "\n",
    "sequences= torch.randn(4, 12 ).cuda() #conditioning sequence; note, max_text_len=12, \n",
    "output=torch.randint (0,logits_dim, (4,  23)).cuda().long() #batch, length (length is flexible)\n",
    "print (output.shape) #(4, 23)\n",
    "\n",
    "loss=model(\n",
    "          sequences=sequences,#conditioning sequence\n",
    "          output=output,\n",
    "          text_mask = None,\n",
    "          return_loss = True,\n",
    "          )\n",
    "loss.backward()\n",
    "loss\n",
    "\n",
    "#if no start token provided: Model will randomly select one\n",
    "generated = model.generate(    sequences=sequences,#conditioning\n",
    "        tokens_to_generate=32, #can also generate less....\n",
    "        cond_scale = 1., #temperature=3,  \n",
    "        )  \n",
    "     \n",
    "#Generate start token\n",
    "output_start=torch.randint (0,logits_dim, (4,  1)).cuda().long() #batch, length (length is flexible)\n",
    "\n",
    "generated = model.generate(sequences=sequences,#conditioning\n",
    "                           output=output_start, #this is the sequence to start with...\n",
    "                           tokens_to_generate=32, #can also generate less....\n",
    "                           cond_scale = 1., temperature=1,  \n",
    "                           )  \n",
    "print (generated.shape) #(b, tokens_to_generate+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587694e-d8fb-4020-ac0e-ec8aa918fb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
